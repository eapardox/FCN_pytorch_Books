# -*- coding: utf-8 -*-
"""Collaborative Filtering with Pytorch_libros.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NfgqB2p1hRnwEzDkLx8-q0u1-r4dRaBC
"""

import pandas as pd
import numpy as np
from sklearn import model_selection, metrics, preprocessing
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

pd.set_option('display.max_columns', None)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

df = pd.read_csv('ratings.csv', sep=';')
df.head()

df.info() # Esquema del dataset

df.book_id.nunique()

df.rating.value_counts() # Distribución del rating

df.shape

"""#### Datos de entrenamiento"""

class BookDataset:
    def __init__(self, users, books, ratings):
        self.users = users
        self.books = books
        self.ratings = ratings

    def __len__(self):
        return len(self.users)

    def __getitem__(self, item):

        users = self.users[item]
        books = self.books[item]
        ratings = self.ratings[item]

        return {
            "users": torch.tensor(users, dtype=torch.long),
            "books": torch.tensor(books, dtype=torch.long),
            "ratings": torch.tensor(ratings, dtype=torch.long),
        }

"""#### Crear el modelo"""

class RecSysModel(nn.Module):
    def __init__(self, n_users, n_books):
        super().__init__()
        # Matriz de búsqueda entrenable para vectores de incrustación poco profundos

        self.user_embed = nn.Embedding(n_users, 32)
        self.book_embed = nn.Embedding(n_books, 32)
        # user, booksr embedding concat
        self.out = nn.Linear(64, 1)


    def forward(self, users, books, ratings=None):
        user_embeds = self.user_embed(users)
        book_embeds = self.book_embed(books)
        output = torch.cat([user_embeds, book_embeds], dim=1)

        output = self.out(output)

        return output

# Codificar el user y el id del libro para empezar desde 0 y no encontarnos con un índice fuera de límite con Embedding
lbl_user = preprocessing.LabelEncoder()
lbl_book = preprocessing.LabelEncoder()
df.user_id = lbl_user.fit_transform(df.user_id.values)
df.book_id = lbl_book.fit_transform(df.book_id.values)

df_train, df_valid = model_selection.train_test_split(
    df, test_size=0.1, random_state=42, stratify=df.rating.values
)

train_dataset = BookDataset(
    users=df_train.user_id.values,
    books=df_train.book_id.values,
    ratings=df_train.rating.values
)

valid_dataset = BookDataset(
    users=df_valid.user_id.values,
    books=df_valid.book_id.values,
    ratings=df_valid.rating.values
)

train_loader = DataLoader(dataset=train_dataset,
                          batch_size=2,
                          shuffle=True,
                          num_workers=2)

validation_loader = DataLoader(dataset=valid_dataset,
                          batch_size=2,
                          shuffle=True,
                          num_workers=2)

dataiter = iter(train_loader)
dataloader_data = next(dataiter)
print(dataloader_data)

model = RecSysModel(
    n_users=len(lbl_user.classes_),
    n_books=len(lbl_book.classes_),
    ).to(device)

optimizer = torch.optim.Adam(model.parameters())
sch = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.4)

loss_func = nn.MSELoss()

print(len(lbl_user.classes_))
print(len(lbl_book.classes_))
print(df.book_id.max())
print(len(train_dataset)) # Datos de entrenamiento

"""#### Ejecutar manualmente una ruta de avance"""

print(dataloader_data['users'])
print(dataloader_data['users'].size())

print(dataloader_data['books'] )
print(dataloader_data['books'].size())

user_embed = nn.Embedding(len(lbl_user.classes_), 32)
book_embed = nn.Embedding(len(lbl_book.classes_), 32)

out = nn.Linear(64, 1)

user_embeds = user_embed(dataloader_data['users'])
book_embeds = book_embed(dataloader_data['books'])
print(f"user_embeds {user_embeds.size()}")
print(f"user_embeds {user_embeds}")
print(f"book_embeds {book_embeds.size()}")
print(f"book_embeds {book_embeds}")

output = torch.cat([ user_embeds, book_embeds], dim=1)
print(f"output: {output.size()}")
print(f"output: {output}")
output = out(output)
print(f"output: {output}")

with torch.no_grad():
    model_output = model(dataloader_data['users'],
                         dataloader_data["books"])

    print(f"model_output: {model_output}, size: {model_output.size()}")

rating = dataloader_data["ratings"]
print(rating)
print(rating.view(2, -1))
print(model_output)

print(rating.sum())

print(model_output.sum() - rating.sum())

"""#### Ejecutar el entrenamineto"""

epochs = 3
total_loss = 0
plot_steps, print_steps = 10000, 10000
step_cnt = 0
all_losses_list = []

model.train()
for epoch_i in range(epochs):
    for i, train_data in enumerate(train_loader):
        output = model(train_data["users"],
                       train_data["books"]
                       )

        # .view(4, -1) es para remodelar la calificación para que coincida con la forma de la salida del modelo que es 4x1
        rating = train_data["ratings"].view(2, -1).to(torch.float32)

        loss = loss_func(output, rating)
        total_loss = total_loss + loss.sum().item()
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        step_cnt = step_cnt + len(train_data["users"])


        if(step_cnt % plot_steps == 0):
            avg_loss = total_loss/(len(train_data["users"]) * plot_steps)
            print(f"epoch {epoch_i} loss at step: {step_cnt} is {avg_loss}")
            all_losses_list.append(avg_loss)
            total_loss = 0 # reset total_loss

plt.figure()
plt.plot(all_losses_list)
plt.show()

"""##### Evaluation with RMSE

"""

from sklearn.metrics import mean_squared_error

model_output_list = []
target_rating_list = []

model.eval()

with torch.no_grad():
    for i, batched_data in enumerate(validation_loader):
        model_output = model(batched_data['users'],
                             batched_data["books"])

        model_output_list.append(model_output.sum().item() / len(batched_data['users']) )

        target_rating = batched_data["ratings"]

        target_rating_list.append(target_rating.sum().item() / len(batched_data['users']))

        print(f"model_output: {model_output}, target_rating: {target_rating}")


# squared If True returns MSE value, if False returns RMSE value.
rms = mean_squared_error(target_rating_list, model_output_list, squared=False)
print(f"rms: {rms}")

"""##### Evaluation with Recall@K

"""

from collections import defaultdict

# un dict que almacena una lista de pares de valoraciones previstas y reales para cada usuario
user_est_true = defaultdict(list)

# iterar a través de los datos de validación para construir el usuario-> [(y1, y1_hat), (y2, y2_hat)...]
with torch.no_grad():
    for i, batched_data in enumerate(validation_loader):
        users = batched_data['users']
        books = batched_data['books']
        ratings = batched_data['ratings']

        model_output = model(batched_data['books'], batched_data["users"])

        for i in range(len(users)):
            user_id = users[i].item()
            book_id = books[i].item()
            pred_rating = model_output[i][0].item()
            true_rating = ratings[i].item()

            print(f"{user_id}, {book_id}, {pred_rating}, {true_rating}")
            user_est_true[user_id].append((pred_rating, true_rating))

with torch.no_grad():
    precisions = dict()
    recalls = dict()

    k=10
    threshold=3.5

    for uid, user_ratings in user_est_true.items():

        # Ordenar las valoraciones de los usuarios por valor estimado.
        user_ratings.sort(key=lambda x: x[0], reverse=True)

        # Obtenemos el número real de artículos relevante
        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)

        # Obtenemos el número de artículos recomendados que se predicen como relevantes y dentro del topk
        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])

        # Obtenemos el número de peliculas comentadas que también son realmente relevantes dentro de topk
        n_rel_and_rec_k = sum(
            ((true_r >= threshold) and (est >= threshold))
            for (est, true_r) in user_ratings[:k]
        )

        print(f"uid {uid},  n_rel {n_rel}, n_rec_k {n_rec_k}, n_rel_and_rec_k {n_rel_and_rec_k}")

        # Precision@K: Proporción de elementos recomendados que son relevantes
        # Donde n_rec_k is 0, Precision es definido, aqui lo ponemos en 0.

        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0

        # Recall@K: Proporción de elementos relevantes que se recomiendan
        # Donde n_rel es 0, Recall es definido. Aquí lo ponemos a 0.

        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0

# Precision and recall se puede promediar entre todos los usuarios
print(f"precision @ {k}: {sum(prec for prec in precisions.values()) / len(precisions)}")

print(f"recall @ {k} : {sum(rec for rec in recalls.values()) / len(recalls)}")

